{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScraper_201701403.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zivkcmD4g5-0",
        "colab_type": "code",
        "outputId": "ea15be43-e637-47a0-cf5b-7c614efdabd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import operator\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "Tag_Rank = {}\n",
        "\n",
        "\n",
        "def ques_links_crawler(base_url, end_url, page_limit,location):\n",
        "  page_no = 1\n",
        "  while page_no <= page_limit:\n",
        "    page_url = base_url + str(page_no) + end_url + location\n",
        "    print(page_url)\n",
        "    source_code = requests.get(page_url).text\n",
        "    soup = BeautifulSoup(source_code, 'html.parser')\n",
        "    if page_no is 1:\n",
        "        os.system('clear')\n",
        "    print('crawling page ' + str(page_no) + ': ', end='')\n",
        "    prev_len = 0\n",
        "    q_no = 1\n",
        "\t  \n",
        "    for ques_link in soup.find_all('div', {'class': 'ps-relative d-inline-block z-selected'}):\n",
        "      for t in ques_link.find_all('a',{'class':'post-tag job-link no-tag-menu'}):\n",
        "        tag = t.text\n",
        "        if tag in Tag_Rank:\n",
        "          Tag_Rank[tag] += 1\n",
        "        else:\n",
        "          Tag_Rank[tag] = 1\n",
        "    page_no += 1\n",
        "\n",
        "\n",
        "def start():\n",
        "  page_limit = int(input('Enter no. of pages to crawl : '))\n",
        "  location=(input('Enter location: '))\n",
        "  os.system('clear')\n",
        "  print('starting crawling...')\n",
        "  ques_links_crawler('https://stackoverflow.com/jobs?d=20&u=Km&sort=i&pg=', '&l=', page_limit,location)\n",
        "  fw = open('Tags_frequency.txt', 'w')\n",
        "  for key, value in sorted(Tag_Rank.items(), key=operator.itemgetter(1), reverse=True):\n",
        "    try:\n",
        "        fw.write(key + \" : \" + str(Tag_Rank[key]) + \"\\n\")\n",
        "    except TypeError:\n",
        "        continue\n",
        "  print('\\nResult saved to file Tags_frequency.txt')\n",
        "\n",
        "start()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter no. of pages to crawl : 1\n",
            "Enter location: gandhinagar\n",
            "starting crawling...\n",
            "https://stackoverflow.com/jobs?d=20&u=Km&sort=i&pg=1&l=gandhinagar\n",
            "crawling page 1: \n",
            "Result saved to file Tags_frequency.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}